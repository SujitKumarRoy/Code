# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zAgxH3h7Yq6OZdjs_8vFAFHORe7t0g0m

# **Assignment 2**
"""

import pandas as pd
import numpy as np

#import files into google drive from pc
from google.colab import files
upload = files.upload()

#import files into google drive from pc
import io
df = pd.read_excel(io.BytesIO(upload['Survey-Cleaning.csv.xlsx']))
print(df)

"""# Remove Dummy Values"""

#makes texts become NaN
df['InternetCost'] = pd.to_numeric(df['InternetCost'], errors='coerce')
df['FamilyIncome'] = pd.to_numeric(df['FamilyIncome'],errors='coerce')

#makes number in all text column NaN
df['Area'] = df['Area'].mask(pd.to_numeric(df['Area'], errors='coerce').notna())
df['Device'] = df['Device'].mask(pd.to_numeric(df['Device'], errors= 'coerce').notna())

#replace 0 or dummy values with Nan
df['InternetCost'] = df['InternetCost'].replace(0,np.nan)
df['FamilyIncome'][df['FamilyIncome'] <50] = np.nan 

print(df)

"""# Replace NaN Values"""

#make the NaN fill with mode of the text
df['Area'].fillna(df['Area'].mode()[0], inplace = True) 
df['Device'].fillna(df['Device'].mode()[0], inplace = True)

#fill NaN values with median values grouped by Area
df['InternetCost'] = df['InternetCost'].fillna(df.groupby('Area')['InternetCost'].transform('median')) #replace Nan values with median
df['FamilyIncome'] = df['FamilyIncome'].fillna(df.groupby('Area')['FamilyIncome'].transform('median')) #replace Nan values with median
print(df)

"""# Transform Dataset"""

#used label encoding for all since df['Device] has many catagories
from sklearn.preprocessing import LabelEncoder 
le = LabelEncoder() 
df['Area']= le.fit_transform(df['Area'])
df['Device'] = le.fit_transform(df['Device'])
df['Gender'] = le.fit_transform(df['Gender']) 
print(df)

"""# Normalize The Data"""

#to see if the data has any outliers
#the presence of outlier would determine what method to use to normalize
df.describe()

#std dev of FamilyIncome greater than its mean, so outliers are present.
#Hence Robust Scaler is used 
from sklearn.preprocessing import RobustScaler
robust = RobustScaler()
robust_scaled_df = robust.fit_transform(df)
robust_scaled_df = pd.DataFrame(robust_scaled_df, columns=df.columns)

#final dataset
print(robust_scaled_df)

